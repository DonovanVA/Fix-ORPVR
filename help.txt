Install on Linux/Docker
Usage: 1. bash end2end.sh all (require data_experiment.txt)
       2. bash end2end.sh clippath all
       3. bash end2end.sh clippath model mode

DAVIS 2017 trainval
dataset. This dataset is a collection of video clips of 854
x 480p sizes, which have a wider screen ratio, but we preprocess the dataset as a 4:3 old screen ratio by cropping the
center of frames to size of 640 x 480
https://davischallenge.org/davis2016/code.html

clippath is the path to the foler containing all the images in ImageSets

FIXES:
1) correct version
These are the packages working for masking and inpainting:
mmcv-full                     1.7.2
mmdet                         2.28.2               
mmengine                      0.10.4
If you accidentally install wrong version (pip or mim):

pip uninstall mmdet
pip uninstall mmcv
pip uninstall mmcv-full

pip install -U openmim
mim install mmcv-full

download mmdetection v2.0
https://github.com/open-mmlab/mmdetection/tree/2.x

cd mmdetection
mkdir checkpoints
pip install -v -e .

*DO NOT install mmcv==1.7.2 and the latest version (2.0.0+) as it will lead to keyErrors mmcv.runner and mmcv._ext module not found

2) masking.py
command a single image or a set of image :python masking.py rawdataset/DAVIS_640x480/00000.jpg

a) fix mmdetection keyError replace:
from mmdetection.mmdet.apis import inference_detector,init_detector
with mmdet.apis import inference_detector,init_detector

b) enable CUDA
download the model (branch 2.0):
https://github.com/open-mmlab/mmdetection/blob/2.x/configs/mask2former/README.md

CUDAv11.7
conda install pytorch==2.0.0 torchvision==0.15.0 torchaudio==2.0.0 pytorch-cuda=11.7 -c pytorch -c nvidia
add in cudnn64_8.dll: https://www.dll-files.com/cudnn64_8.dll.html#google_vignette if not found

install tensorflow
tensorflow                    2.10.1
torch                         2.0.0
torchvision                   0.15.0
torchaudio                    2.0.0

c) download util file in 'test clear' commit of main branch (ORPVR SHA eef6dcb39e05041e044e46141db80d37fb93d9b4)
https://github.com/jinjungyu/ORPVR/tree/main


3) inpainting.py:
git clone https://github.com/MCG-NKU/E2FGVI.git
Install the model https://drive.google.com/file/d/10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3/view
put the model in /release_model

a) Download the weights G0000000.pt:
https://drive.google.com/drive/folders/1bSOH-2nB3feFRyDEmiX81CEiWkghss3i
and place it in AOT-GAN-for-Inpainting/experiments/

b) Install the image inpainting model:
git clone https://github.com/hyunobae/AOT-GAN-for-Inpainting.git


COMMANDS:
python image_preprocess.py sample sample_processed --width 640 --height 480 #(no "/", sample is your folder containing raw images)
python masking.py sample_processed #(no "/", sample_processed is your folder containing processed images). saves to /dataset dir
python inpainting.py dataset/sample_processed --model e2fgvi_hq #('aotgan', 'e2fgvi', 'e2fgvi_hq') saves to /result_inpaint dir
python relocating.py result_inpaint/sample_processed/e2fgvi_hq --mode 0 #(0:'original',1:'offset',2:'dynamic') saves to /result dir
python encoding.py result/sample_processed/e2fgvi_hq/original #('original','offset','dynamic') saves to /video dir